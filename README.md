
# ğŸ“Š Telecom Customer Churn Risk Analysis (PySpark + SQL)

This project demonstrates an end-to-end data pipeline and analytical workflow using **PySpark** and **SQL** to analyze **customer churn risk** in a telecom company. It includes data ingestion, cleaning, feature engineering, risk classification, and business-driven SQL analysis.

---

## ğŸš€ Project Highlights

- ğŸ”„ **Data Ingestion & Transformation** using PySpark
- ğŸ§¼ **Data Cleaning** and null handling
- ğŸ”— **Joins across multiple data sources** (customers, usage, complaints)
- ğŸ“Š **Churn Risk Classification** based on usage patterns and complaints
- ğŸ“ˆ **SQL-based Business Analysis** on churn drivers
- âœ… Structured, modular notebooks with markdown explanations

---

## ğŸ“ Project Structure

```
telecom-churn-risk-pyspark/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_data_transformation.ipynb     â† ETL & churn risk logic
â”‚   â””â”€â”€ 2_churn_sql_analysis.ipynb      â† SQL insights on churn data
â”œâ”€â”€ data/
â”‚   â””â”€â”€ churn_final.csv                 â† Cleaned, labeled dataset
â”œâ”€â”€ README.md
```

---

## ğŸ“Š Dataset Overview

We used synthetic telecom data including:

- **Customers Data** (`customer_id`, `age`, `plan_type`, `city`, `join_date`)
- **Usage Data** (`calls_made`, `data_used_gb`, `month`)
- **Complaints Data** (`complaint_type`, `complaint_date`)

All data was cleaned, joined, and enriched in PySpark.

---

## ğŸ§  Churn Risk Logic

Each customer was labeled as **Low**, **Medium**, or **High** churn risk based on:

- Average monthly **calls** and **data usage**
- Number of **complaints**
- Plan type and tenure

The risk logic was implemented using `when` conditions and feature engineering in PySpark.

---

## â“ Sample SQL Business Questions Answered

- What is the overall churn risk distribution?
- Does age impact churn risk?
- Which cities have the most high-risk customers?
- What plan types are most vulnerable to churn?
- Does customer usage pattern correlate with churn?

ğŸ“Œ All SQL queries are run via `createOrReplaceTempView()` in Spark.

---

## ğŸ’» Tech Stack

- ğŸ”¥ **PySpark** for data transformation & feature engineering
- ğŸ§ª **Spark SQL** for business analysis
- ğŸ“„ CSV-based input/output
- ğŸ“ Jupyter Notebooks for code & documentation

---

## ğŸ“š Future Enhancements

- Add interactive dashboards using Power BI or Tableau
- Apply ML models for churn **prediction** (Logistic Regression, XGBoost)
- Automate the pipeline using Apache Airflow or ADF

---

## ğŸ™‹â€â™€ï¸ About Me

ğŸ‘©â€ğŸ’» Iâ€™m Anjali Mane, a Data Engineer with experience in Azure Data Factory, Synapse Analytics, and Python. I built this project to demonstrate practical data transformation and analysis skills.

ğŸ“¬ Letâ€™s connect on [LinkedIn](https://www.linkedin.com/in/your-link/)  
ğŸŒŸ If you found this helpful, feel free to star the repo!
